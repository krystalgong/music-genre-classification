{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name(file_dir):   \n",
    "    L=[]   \n",
    "    for root, dirs, files in os.walk(file_dir):  \n",
    "        for file in files:  \n",
    "            if os.path.splitext(file)[1] == '.h5':\n",
    "                L.append(os.path.join(root, file)[29:-3])  \n",
    "    return L # file name = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_genre_1(lst):\n",
    "    f = open('genre.txt','r')\n",
    "    dic = {}\n",
    "    Set = []\n",
    "    id_genre = {}\n",
    "    for i in f.readlines():\n",
    "        content = i.split()\n",
    "        content[-1] = content[-1][:-1]\n",
    "        if len(content) == 3:\n",
    "            content[1] = content[1]+\" \"+content[2]\n",
    "        if len(content) == 1:\n",
    "            continue\n",
    "        dic[content[0]] = content[1]\n",
    "        Set.append(content[1])\n",
    "    genre = list(set(Set))\n",
    "    # print(genre)\n",
    "    # print(len(set(Set)))\n",
    "    number = [0]*15\n",
    "    for j in lst:\n",
    "        if j in dic:\n",
    "            id_genre[j] = dic[j]\n",
    "    for m in id_genre:\n",
    "        number[genre.index(id_genre[m])] +=1\n",
    "    # print(number)\n",
    "    # print(sum(number))\n",
    "    return id_genre, genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_genre_2(lst,id_genre):\n",
    "    g = open('newgenre.txt','r')\n",
    "    dic2 = {}\n",
    "    # Set2 = []\n",
    "    for k in g.readlines():\n",
    "        content = k.split()\n",
    "        if len(content) == 3:\n",
    "            content[1] = content[1]+\" \"+content[2]\n",
    "        if len(content) == 1:\n",
    "            continue\n",
    "        dic2[content[0]] = content[1]\n",
    "    #     Set2.append(content[1])\n",
    "    # genre2 = list(set(Set2))\n",
    "    # print(genre2)\n",
    "    # print(len(set(Set2)))\n",
    "    for item in lst:\n",
    "        if (item not in id_genre) and (item in dic2) and dic2[item] != \"Pop_Rock\" and dic2[item] != \"International\" and dic2[item] != \"Vocal\":\n",
    "            id_genre[item] = dic2[item]\n",
    "    # print(len(id_genre))\n",
    "    return id_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_process(updated_id_genre):\n",
    "    processed_data = {}\n",
    "    x = []\n",
    "    y = []\n",
    "    lenlist = []\n",
    "    for k,v in updated_id_genre.items():\n",
    "        lst = []\n",
    "        for root, dirs, files in os.walk(\"MillionSongSubset/data\"):\n",
    "            for file in files:\n",
    "                if os.path.splitext(file)[0] == k:\n",
    "                    f = h5py.File(os.path.join(root, file), 'r')\n",
    "                    for i in f['analysis'][\"segments_timbre\"]:\n",
    "                        for m in list(i):\n",
    "                            lst.append(m)\n",
    "        if len(lst)>=4800:\n",
    "            Array = np.array(lst[:4800])\n",
    "            processed_data[k] = [v,Array]\n",
    "            x.append(Array)\n",
    "            y.append(v)\n",
    "    return processed_data, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3396\n"
     ]
    }
   ],
   "source": [
    "lst = file_name(\"MillionSongSubset/data\")\n",
    "\n",
    "id_genre, genre = open_genre_1(lst)\n",
    "# genre = ['Rock', 'Country', 'New Age', 'Jazz', 'Folk', 'Blues', 'Pop', 'Metal', 'World', 'Reggae', 'Electronic', 'Punk', 'Rap', 'RnB', 'Latin']\n",
    "updated_id_genre = open_genre_2(lst,id_genre)\n",
    "print(len(updated_id_genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190\n",
      "3190\n"
     ]
    }
   ],
   "source": [
    "processed_data, x, y= data_pre_process(updated_id_genre)\n",
    "print(len(processed_data))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.array(x).reshape(3190,12,20,20)\n",
    "y_number = [genre.index(y[i]) for i in range(len(y))]\n",
    "y_all = np.array(y_number)\n",
    "X_train = X_all[:1400]\n",
    "y_train = y_all[:1400]\n",
    "X_val = X_all[1400:1590]\n",
    "y_val = y_all[1400:1590]\n",
    "X_test = X_all[1590:]\n",
    "y_test = y_all[1590:]\n",
    "# X_train = X_all[:2000]\n",
    "# y_train = y_all[:2000]\n",
    "# X_val = X_all[2000:2190]\n",
    "# y_val = y_all[2000:2190]\n",
    "# X_test = X_all[2190:]\n",
    "# y_test = y_all[2190:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 32, 3, 1, 1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.fc1 = nn.Linear(64 * 20 * 20, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 15)\n",
    "        #Three linear layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x  = x.view(-1,64 * 20 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model structure:  ConvNet(\n",
      "  (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=25600, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=15, bias=True)\n",
      ")\n",
      "X train tensor shape: torch.Size([1400, 12, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# init network\n",
    "conv_net = ConvNet()\n",
    "print('model structure: ', conv_net)\n",
    "# init optimizer\n",
    "optimizer = optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# prepare for mini-batch stochastic gradient descent\n",
    "n_iteration = 20\n",
    "batch_size = 32\n",
    "n_data = X_train.shape[0]\n",
    "n_batch = int(np.ceil(n_data/batch_size))\n",
    "\n",
    "# convert X_train and X_val to tensor and flatten them\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_val_tensor = torch.Tensor(X_val)\n",
    "\n",
    "# convert training label to tensor and to type long\n",
    "y_train_tensor = torch.Tensor(y_train).long()\n",
    "y_val_tensor = torch.Tensor(y_val).long()\n",
    "\n",
    "print('X train tensor shape:', X_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_and_accuracy(y_pred, y):\n",
    "    # y_pred is the nxC prediction scores\n",
    "    # give the number of correct and the accuracy\n",
    "    n = y.shape[0]\n",
    "    # find the prediction class label\n",
    "    _ ,pred_class = y_pred.max(dim=1)\n",
    "    correct = (pred_class == y).sum().item()\n",
    "    return correct ,correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 ,Train loss: 3.240, Train acc: 0.361, Val loss: 2.291, Val acc: 0.316\n",
      "Iter 1 ,Train loss: 1.918, Train acc: 0.417, Val loss: 2.031, Val acc: 0.389\n",
      "Iter 2 ,Train loss: 1.327, Train acc: 0.587, Val loss: 2.039, Val acc: 0.395\n",
      "Iter 3 ,Train loss: 0.541, Train acc: 0.859, Val loss: 2.285, Val acc: 0.379\n",
      "Iter 4 ,Train loss: 0.190, Train acc: 0.947, Val loss: 2.460, Val acc: 0.353\n",
      "Iter 5 ,Train loss: 0.161, Train acc: 0.952, Val loss: 3.283, Val acc: 0.258\n",
      "Iter 6 ,Train loss: 0.124, Train acc: 0.970, Val loss: 2.690, Val acc: 0.411\n",
      "Iter 7 ,Train loss: 0.081, Train acc: 0.985, Val loss: 4.277, Val acc: 0.395\n",
      "Iter 8 ,Train loss: 0.045, Train acc: 0.991, Val loss: 3.276, Val acc: 0.416\n",
      "Iter 9 ,Train loss: 0.008, Train acc: 0.998, Val loss: 3.573, Val acc: 0.432\n",
      "Iter 10 ,Train loss: 0.012, Train acc: 0.998, Val loss: 3.729, Val acc: 0.432\n",
      "Iter 11 ,Train loss: 0.001, Train acc: 1.000, Val loss: 4.071, Val acc: 0.395\n",
      "Iter 12 ,Train loss: 0.001, Train acc: 1.000, Val loss: 4.019, Val acc: 0.400\n",
      "Iter 13 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.125, Val acc: 0.395\n",
      "Iter 14 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.199, Val acc: 0.395\n",
      "Iter 15 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.258, Val acc: 0.395\n",
      "Iter 16 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.310, Val acc: 0.395\n",
      "Iter 17 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.360, Val acc: 0.400\n",
      "Iter 18 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.403, Val acc: 0.400\n",
      "Iter 19 ,Train loss: 0.000, Train acc: 1.000, Val loss: 4.444, Val acc: 0.400\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = np.zeros(n_iteration)\n",
    "train_accu_list = np.zeros(n_iteration)\n",
    "val_loss_list = np.zeros(n_iteration)\n",
    "val_accu_list = np.zeros(n_iteration)\n",
    "for i in range(n_iteration):\n",
    "    # first get a minibatch of data\n",
    "    running_loss = 0\n",
    "    correct_train = 0\n",
    "    for j in range(n_batch):\n",
    "        batch_start_index = j*batch_size\n",
    "        # get data batch from the normalized data\n",
    "        X_batch = X_train_tensor[batch_start_index:batch_start_index+batch_size]\n",
    "        # get ground truth label y\n",
    "        y_batch = y_train_tensor[batch_start_index:batch_start_index+batch_size]\n",
    "        pred = conv_net(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        running_loss += loss.item()\n",
    "        correct_train += get_correct_and_accuracy(pred, y_batch)[0]\n",
    "        # update \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    ave_train_loss = running_loss / n_batch\n",
    "    pred_val = conv_net(X_val_tensor)\n",
    "    loss_val = criterion(pred_val, y_val_tensor)\n",
    "    val_loss = loss_val\n",
    "    train_accu = correct_train/float(n_data)\n",
    "    val_accu =  get_correct_and_accuracy(pred_val, y_val_tensor)[1]\n",
    "\n",
    "    print(\"Iter %d ,Train loss: %.3f, Train acc: %.3f, Val loss: %.3f, Val acc: %.3f\" \n",
    "          %(i ,ave_train_loss, train_accu, val_loss, val_accu)) \n",
    "    ## add to the logs so that we can use them later for plotting\n",
    "    train_loss_list[i] = ave_train_loss\n",
    "    train_accu_list[i] = train_accu\n",
    "    val_loss_list[i] = val_loss\n",
    "    val_accu_list[i] = val_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.25625\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.Tensor(X_test)\n",
    "y_test_tensor = torch.Tensor(y_test).long()\n",
    "pred = conv_net(X_test_tensor)\n",
    "correct_train = get_correct_and_accuracy(pred, y_test_tensor)[1]\n",
    "print(\"Test accuracy: \", correct_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 4800)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 1000)\n",
    "X_pca = X_all.reshape(3190,4800)\n",
    "pca.fit(X_pca)\n",
    "Data = X_pca\n",
    "print(Data.shape)\n",
    "X_train_pca=Data[:1500]\n",
    "X_val_pca=Data[1500:1690]\n",
    "X_test_pca=Data[1690:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1500, 1400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f805e3333f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    144\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    145\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1500, 1400]"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(X_val_pca)):\n",
    "    if clf.predict(X_val_pca[i].reshape(1,-1))==y_val[i]:\n",
    "        correct +=1\n",
    "correct_rate = correct/len(y_val)\n",
    "print(correct_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "print(len(X_test_pca))\n",
    "for i in range(len(X_test_pca)):\n",
    "    a = clf.predict(X_test_pca[i].reshape(1,-1))\n",
    "    if a==y_test[i]:\n",
    "        correct +=1\n",
    "correct_rate = correct/len(y_test)\n",
    "print(correct_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
